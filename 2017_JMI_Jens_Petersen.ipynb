{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journal of Medical Imaging Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission title:\n",
    "> Effective User Interaction in Online Interactive Semantic Segmentation for Glioblastoma MRI<br>\n",
    "> Jens Petersen, Martin Bendszus, JÃ¼rgen Debus, Sabine Heiland, Klaus H. Maier-Hein\n",
    "\n",
    "Methods:\n",
    "1. UNCERTAIN\n",
    "2. MISCLASS\n",
    "3. MISCLASS-B\n",
    "4. UNCERTAIN-MB\n",
    "5. CERTAIN-MB\n",
    "\n",
    "For description of methods etc. please see publication.\n",
    "\n",
    "Author of this document ist Jens Petersen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xarray as xr\n",
    "from util import dataio, interactive, metrics, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"labels\" : [1, 2, 3, 4, (1, 2, 3, 4)],\n",
    "    \"label_names\" : [\"Tumor Core\", \"Edema\", \"Non-enhancing Abnormality\", \"Enhancing Tumor\", \"Whole Tumor\"],\n",
    "    \"subjects\" : [\"HG0001\", \"HG0002\", \"HG0003\", \"HG0004\", \"HG0005\", \"HG0006\", \"HG0007\", \"HG0008\", \"HG0009\", \"HG0010\",\n",
    "                  \"HG0011\", \"HG0012\", \"HG0013\", \"HG0014\", \"HG0015\", \"HG0022\", \"HG0024\", \"HG0025\", \"HG0026\", \"HG0027\"],\n",
    "    \"methods\" : [\"UNCERTAIN\", \"MISCLASS\", \"MISCLASS-B\", \"UNCERTAIN-MB\", \"CERTAIN-MB\"],\n",
    "    \"epochs\" : 50,\n",
    "    \"repetitions\" : 5,\n",
    "    \"uncertainty_threshold\" : 0.8,\n",
    "    \"stroke_length\" : 10,\n",
    "    \"uncertainty_type\" : \"entropy\",  # entropy, inv_margin, inv_confidence\n",
    "    \"uncertainty_func\" : metrics.entropy,\n",
    "    \"start_indices_type\" : \"weighted\",  # weighted, random\n",
    "    \"n_start_indices\" : 50\n",
    "}\n",
    "\n",
    "forest_config = {\n",
    "    \"n_estimators\": 50,\n",
    "    \"max_depth\": 10,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "data_locations = []\n",
    "truth_locations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UNCERTAIN(data, truth, config, forest_config):\n",
    "    \n",
    "    # shapes\n",
    "    base_shape = truth.shape\n",
    "    n_features = data.shape[-1]\n",
    "    \n",
    "    # points for initialization\n",
    "    if config[\"start_indices_type\"] == \"weighted\":\n",
    "        start_indices = interactive.weighted_start_indices(truth, config[\"n_start_indices\"])\n",
    "    elif config[\"start_indices_type\"] == \"random\":\n",
    "        start_indices = interactive.random_indices(base_shape, config[\"n_start_indices\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option for start_indices_type.\")\n",
    "        \n",
    "    # construct training data\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for index in start_indices:\n",
    "        training_data.append(data[index])\n",
    "        training_labels.append(truth[index])\n",
    "    \n",
    "    # initialize forest\n",
    "    rf = RandomForestClassifier(**forest_config)\n",
    "    rf.fit(training_data, training_labels)\n",
    "    probabilities = dataio.inflate(rf.predict_proba(dataio.flat(data)), base_shape)\n",
    "    uncertainty = config[\"uncertainty_func\"](probabilities)\n",
    "    mask = interactive.maximum_uncertainty_region(uncertainty,\n",
    "        **parse.config_for_function(interactive.maximum_uncertainty_region, config))\n",
    "    \n",
    "    # evaluation\n",
    "    eval_suite = metrics.EvaluationSuite(reference=truth, labels=config[\"labels\"], label_names=config[\"label_names\"])\n",
    "    scores = []\n",
    "        \n",
    "    # do experiment\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        \n",
    "        try:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], mask=mask, groundtruth=truth)\n",
    "        except IndexError:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], groundtruth=truth)\n",
    "        for index in current_indices:\n",
    "            training_data.append(data[index])\n",
    "            training_labels.append(truth[index])\n",
    "            \n",
    "        rf.fit(training_data, training_labels)\n",
    "        probabilities = rf.predict_proba(dataio.flat(data))\n",
    "        segmentation = rf.classes_.take(np.argmax(probabilities, axis=1), axis=0).reshape(base_shape)\n",
    "        probabilities = dataio.inflate(probabilities, base_shape)\n",
    "        uncertainty = config[\"uncertainty_func\"](probabilities)\n",
    "        mask = interactive.maximum_uncertainty_region(uncertainty,\n",
    "            **parse.config_for_function(interactive.maximum_uncertainty_region, config))\n",
    "        \n",
    "        eval_suite.set_test(segmentation)\n",
    "        eval_suite.evaluate()\n",
    "        scores.append(eval_suite.to_pandas())\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def MISCLASS(data, truth, config, forest_config):\n",
    "    \n",
    "    # shapes\n",
    "    base_shape = truth.shape\n",
    "    n_features = data.shape[-1]\n",
    "    \n",
    "    # points for initialization\n",
    "    if config[\"start_indices_type\"] == \"weighted\":\n",
    "        start_indices = interactive.weighted_start_indices(truth, config[\"n_start_indices\"])\n",
    "    elif config[\"start_indices_type\"] == \"random\":\n",
    "        start_indices = interactive.random_indices(base_shape, config[\"n_start_indices\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option for start_indices_type.\")\n",
    "        \n",
    "    # construct training data\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for index in start_indices:\n",
    "        training_data.append(data[index])\n",
    "        training_labels.append(truth[index])\n",
    "    \n",
    "    # initialize forest\n",
    "    rf = RandomForestClassifier(**forest_config)\n",
    "    rf.fit(training_data, training_labels)\n",
    "    segmentation = rf.predict(dataio.flat(data)).reshape(base_shape)\n",
    "    mask = segmentation != truth\n",
    "    \n",
    "    # evaluation\n",
    "    eval_suite = metrics.EvaluationSuite(reference=truth, labels=config[\"labels\"], label_names=config[\"label_names\"])\n",
    "    scores = []\n",
    "        \n",
    "    # do experiment\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        \n",
    "        try:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], mask=mask, groundtruth=truth)\n",
    "        except IndexError:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], groundtruth=truth)\n",
    "        for index in current_indices:\n",
    "            training_data.append(data[index])\n",
    "            training_labels.append(truth[index])\n",
    "            \n",
    "        rf.fit(training_data, training_labels)\n",
    "        segmentation = rf.predict(dataio.flat(data)).reshape(base_shape)\n",
    "        mask = segmentation != truth\n",
    "        \n",
    "        eval_suite.set_test(segmentation)\n",
    "        eval_suite.evaluate()\n",
    "        scores.append(eval_suite.to_pandas())\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def MISCLASS_B(data, truth, config, forest_config):\n",
    "    \n",
    "    # shapes\n",
    "    base_shape = truth.shape\n",
    "    n_features = data.shape[-1]\n",
    "    \n",
    "    # points for initialization\n",
    "    if config[\"start_indices_type\"] == \"weighted\":\n",
    "        start_indices = interactive.weighted_start_indices(truth, config[\"n_start_indices\"])\n",
    "    elif config[\"start_indices_type\"] == \"random\":\n",
    "        start_indices = interactive.random_indices(base_shape, config[\"n_start_indices\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option for start_indices_type.\")\n",
    "        \n",
    "    # construct training data\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for index in start_indices:\n",
    "        training_data.append(data[index])\n",
    "        training_labels.append(truth[index])\n",
    "    \n",
    "    # initialize forest\n",
    "    rf = RandomForestClassifier(**forest_config)\n",
    "    rf.fit(training_data, training_labels)\n",
    "    segmentation = rf.predict(dataio.flat(data)).reshape(base_shape)\n",
    "    mask = segmentation != truth\n",
    "    \n",
    "    # evaluation\n",
    "    eval_suite = metrics.EvaluationSuite(reference=truth, labels=config[\"labels\"], label_names=config[\"label_names\"])\n",
    "    scores = []\n",
    "        \n",
    "    # do experiment\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        \n",
    "        # random label from mask\n",
    "        random_label = random.choice(np.unique(truth))\n",
    "        mask *= (segmentation == random_label) + (truth == random_label)\n",
    "        \n",
    "        try:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], mask=mask, groundtruth=truth)\n",
    "        except IndexError:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], groundtruth=truth)\n",
    "        for index in current_indices:\n",
    "            training_data.append(data[index])\n",
    "            training_labels.append(truth[index])\n",
    "            \n",
    "        rf.fit(training_data, training_labels)\n",
    "        segmentation = rf.predict(dataio.flat(data)).reshape(base_shape)\n",
    "        mask = segmentation != truth\n",
    "        \n",
    "        eval_suite.set_test(segmentation)\n",
    "        eval_suite.evaluate()\n",
    "        scores.append(eval_suite.to_pandas())\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def UNCERTAIN_MB(data, truth, config, forest_config):\n",
    "    \n",
    "    # shapes\n",
    "    base_shape = truth.shape\n",
    "    n_features = data.shape[-1]\n",
    "    \n",
    "    # points for initialization\n",
    "    if config[\"start_indices_type\"] == \"weighted\":\n",
    "        start_indices = interactive.weighted_start_indices(truth, config[\"n_start_indices\"])\n",
    "    elif config[\"start_indices_type\"] == \"random\":\n",
    "        start_indices = interactive.random_indices(base_shape, config[\"n_start_indices\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option for start_indices_type.\")\n",
    "        \n",
    "    # construct training data\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for index in start_indices:\n",
    "        training_data.append(data[index])\n",
    "        training_labels.append(truth[index])\n",
    "    \n",
    "    # initialize forest\n",
    "    rf = RandomForestClassifier(**forest_config)\n",
    "    rf.fit(training_data, training_labels)\n",
    "    probabilities = rf.predict_proba(dataio.flat(data))\n",
    "    segmentation = rf.classes_.take(np.argmax(probabilities, axis=1), axis=0).reshape(base_shape)\n",
    "    probabilities = dataio.inflate(probabilities, base_shape)\n",
    "    uncertainty = config[\"uncertainty_func\"](probabilities)\n",
    "    mask = segmentation != truth\n",
    "    \n",
    "    # evaluation\n",
    "    eval_suite = metrics.EvaluationSuite(reference=truth, labels=config[\"labels\"], label_names=config[\"label_names\"])\n",
    "    scores = []\n",
    "        \n",
    "    # do experiment\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        \n",
    "        # random label from mask\n",
    "        random_label = random.choice(np.unique(truth))\n",
    "        mask *= (segmentation == random_label) + (truth == random_label)\n",
    "        \n",
    "        # uncertainty range of mask\n",
    "        min_uncertainty = np.min(uncertainty[mask])\n",
    "        max_uncertainty = np.max(uncertainty[mask])\n",
    "        mask *= uncertainty > (config[\"uncertainty_threshold\"] * (max_uncertainty - min_uncertainty) + min_uncertainty)\n",
    "        if not np.any(mask):\n",
    "            mask = np.ones(base_shape, dtype=np.bool)\n",
    "        \n",
    "        try:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], mask=mask, groundtruth=truth)\n",
    "        except IndexError:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], groundtruth=truth)\n",
    "        for index in current_indices:\n",
    "            training_data.append(data[index])\n",
    "            training_labels.append(truth[index])\n",
    "            \n",
    "        rf.fit(training_data, training_labels)\n",
    "        probabilities = rf.predict_proba(dataio.flat(data))\n",
    "        segmentation = rf.classes_.take(np.argmax(probabilities, axis=1), axis=0).reshape(base_shape)\n",
    "        probabilities = dataio.inflate(probabilities, base_shape)\n",
    "        uncertainty = config[\"uncertainty_func\"](probabilities)\n",
    "        mask = segmentation != truth\n",
    "        \n",
    "        eval_suite.set_test(segmentation)\n",
    "        eval_suite.evaluate()\n",
    "        scores.append(eval_suite.to_pandas())\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def CERTAIN_MB(data, truth, config, forest_config):\n",
    "    \n",
    "    # shapes\n",
    "    base_shape = truth.shape\n",
    "    n_features = data.shape[-1]\n",
    "    \n",
    "    # points for initialization\n",
    "    if config[\"start_indices_type\"] == \"weighted\":\n",
    "        start_indices = interactive.weighted_start_indices(truth, config[\"n_start_indices\"])\n",
    "    elif config[\"start_indices_type\"] == \"random\":\n",
    "        start_indices = interactive.random_indices(base_shape, config[\"n_start_indices\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option for start_indices_type.\")\n",
    "        \n",
    "    # construct training data\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for index in start_indices:\n",
    "        training_data.append(data[index])\n",
    "        training_labels.append(truth[index])\n",
    "    \n",
    "    # initialize forest\n",
    "    rf = RandomForestClassifier(**forest_config)\n",
    "    rf.fit(training_data, training_labels)\n",
    "    probabilities = rf.predict_proba(dataio.flat(data))\n",
    "    segmentation = rf.classes_.take(np.argmax(probabilities, axis=1), axis=0).reshape(base_shape)\n",
    "    probabilities = dataio.inflate(probabilities, base_shape)\n",
    "    uncertainty = config[\"uncertainty_func\"](probabilities)\n",
    "    mask = segmentation != truth\n",
    "    \n",
    "    # evaluation\n",
    "    eval_suite = metrics.EvaluationSuite(reference=truth, labels=config[\"labels\"], label_names=config[\"label_names\"])\n",
    "    scores = []\n",
    "        \n",
    "    # do experiment\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        \n",
    "        # random label from mask\n",
    "        random_label = random.choice(np.unique(truth))\n",
    "        mask *= (segmentation == random_label) + (truth == random_label)\n",
    "        if not np.any(mask):\n",
    "            mask = np.ones(base_shape, dtype=np.bool)\n",
    "        \n",
    "        # uncertainty range of mask\n",
    "        min_uncertainty = np.min(uncertainty[mask])\n",
    "        max_uncertainty = np.max(uncertainty[mask])\n",
    "        mask *= uncertainty < ((1 - config[\"uncertainty_threshold\"]) * (max_uncertainty - min_uncertainty) + min_uncertainty)\n",
    "        \n",
    "        try:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], mask=mask, groundtruth=truth)\n",
    "        except IndexError:\n",
    "            current_indices = interactive.random_stroke(base_shape, config[\"stroke_length\"], groundtruth=truth)\n",
    "        for index in current_indices:\n",
    "            training_data.append(data[index])\n",
    "            training_labels.append(truth[index])\n",
    "            \n",
    "        rf.fit(training_data, training_labels)\n",
    "        probabilities = rf.predict_proba(dataio.flat(data))\n",
    "        segmentation = rf.classes_.take(np.argmax(probabilities, axis=1), axis=0).reshape(base_shape)\n",
    "        probabilities = dataio.inflate(probabilities, base_shape)\n",
    "        uncertainty = config[\"uncertainty_func\"](probabilities)\n",
    "        mask = segmentation != truth\n",
    "        \n",
    "        eval_suite.set_test(segmentation)\n",
    "        eval_suite.evaluate()\n",
    "        scores.append(eval_suite.to_pandas())\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = xr.DataArray(np.zeros((len(config[\"methods\"]),\n",
    "                                len(config[\"subjects\"]),\n",
    "                                config[\"repetitions\"],\n",
    "                                config[\"epochs\"],\n",
    "                                len(config[\"label_names\"]),\n",
    "                                len(metrics.EvaluationSuite._metrics))),\n",
    "                      [(\"Methods\", config[\"methods\"]),\n",
    "                       (\"Subjects\", config[\"subjects\"]),\n",
    "                       (\"Repetitions\", range(config[\"repetitions\"])),\n",
    "                       (\"Epochs\", range(config[\"epochs\"])),\n",
    "                       (\"Labels\", config[\"label_names\"]),\n",
    "                       (\"Metrics\", metrics.EvaluationSuite._metrics)],\n",
    "                      name=\"Scores\")\n",
    "\n",
    "for s, subject in enumerate(config[\"subjects\"]):\n",
    "    \n",
    "    current_data = dataio.copy_from_file(data_locations[s], dtype=np.float32)\n",
    "    current_truth = dataio.copy_from_file(truth_locations[s], dtype=np.int)\n",
    "    \n",
    "    for r in range(config[\"repetitions\"]):\n",
    "        for m, method in enumerate(config[\"methods\"]):\n",
    "            \n",
    "            print subject, r, method, \"...\",\n",
    "            \n",
    "            current_scores = eval(method.replace(\"-\", \"_\"))(current_data, current_truth, config, forest_config)\n",
    "            for epoch, score in enumerate(current_scores):\n",
    "                scores.loc[method, subject, r, epoch, :, :] = score.values\n",
    "                \n",
    "            print \"done\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_rep_mean = scores.mean(\"Repetitions\")\n",
    "scores_mean = scores.mean([\"Subjects\", \"Repetitions\"])\n",
    "scores_std = scores_rep_mean.std(\"Subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_func = stats.wilcoxon\n",
    "\n",
    "n_comparisons = sum(range(len(scores_rep_mean[\"Methods\"])))\n",
    "test_scores = np.zeros((n_comparisons,\n",
    "                        len(scores_rep_mean[\"Epochs\"]),\n",
    "                        len(scores_rep_mean[\"Labels\"]),\n",
    "                        len(scores_rep_mean[\"Metrics\"]),\n",
    "                        3))\n",
    "\n",
    "comparison_labels = []\n",
    "for m1, method1 in enumerate(scores_rep_mean[\"Methods\"].values[:-1]):\n",
    "    for m2, method2 in enumerate(scores_rep_mean[\"Methods\"].values[m1+1:]):\n",
    "        \n",
    "        comparison_labels.append(\"{} v {}\".format(method1, method2))\n",
    "        \n",
    "        for e, epoch in enumerate(scores_rep_mean[\"Epochs\"].values):\n",
    "            for l, label in enumerate(scores_rep_mean[\"Labels\"].values):\n",
    "                for m, metric in enumerate(scores_rep_mean[\"Metrics\"].values):\n",
    "                \n",
    "                    test_stat, p = test_func(scores_rep_mean.loc[method1, :, epoch, label, metric],\n",
    "                                             scores_rep_mean.loc[method2, :, epoch, label, metric])\n",
    "                    median_delta = np.median(scores_rep_mean.loc[method1, :, epoch, label, metric]) -\\\n",
    "                                   np.median(scores_rep_mean.loc[method2, :, epoch, label, metric])\n",
    "                    test_scores[len(comparison_labels) - 1, e, l, m] = [test_stat, p, median_delta]\n",
    "                \n",
    "test_scores = xr.DataArray(test_scores,\n",
    "                           [(\"Comparisons\", comparison_labels),\n",
    "                            (\"Epochs\", scores_rep_mean[\"Epochs\"].values),\n",
    "                            (\"Labels\", scores_rep_mean[\"Labels\"].values),\n",
    "                            (\"Metrics\", scores_rep_mean[\"Metrics\"].values),\n",
    "                            (\"Statistics\", [\"Statistic\", \"p\", \"Delta of Medians\"])],\n",
    "                           name=\"T-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "colors = [mpl.cm.viridis(1.*i/len(config[\"methods\"])) for i in range(len(config[\"methods\"]))]\n",
    "styles = [\"--\", \"-\", \"-\", \"--\", \"--\"]\n",
    "linewidth = 1.5\n",
    "y_label = \"Dice Score\"\n",
    "show_metric = \"dice\"\n",
    "method_explanations = [\n",
    "    \"Annotate in most uncertain region\",\n",
    "    \"Randomly correct classifier\",\n",
    "    \"Correct classifier in random class\",\n",
    "    \"Correct classifier in random class\\nwhere most uncertain\",\n",
    "    \"Correct classifier in random class\\nwhere most certain\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dice score over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "n_cols = 2\n",
    "n_rows = 4\n",
    "show_std_for_method = \"MISCLASS-B\"\n",
    "std_color = colors[list(scores_mean[\"Methods\"]).index(show_std_for_method)]\n",
    "std_alpha = 0.2\n",
    "text_positions = [\n",
    "    (2, 0.9, \"left\"),\n",
    "    (4, 0.8, \"left\"),\n",
    "    (4, 0.8, \"left\"),\n",
    "    (4, 0.8, \"left\"),\n",
    "    (45, 0.1, \"right\"),\n",
    "]\n",
    "\n",
    "# initialize figure and gridspec\n",
    "fig = plt.figure(figsize=(8.27, 10))\n",
    "gs0 = mpl.gridspec.GridSpec(2, 1)\n",
    "gs1 = mpl.gridspec.GridSpecFromSubplotSpec(2, 2, subplot_spec=gs0[1], wspace=0, hspace=0)\n",
    "gs0.update(left=0.1, right=0.95, top=0.98, bottom=0.05)\n",
    "\n",
    "# invisible plot to carry axis label\n",
    "bg_ax = fig.add_subplot(gs0[1])\n",
    "for s in bg_ax.spines:\n",
    "    bg_ax.spines[s].set_color(\"none\")\n",
    "bg_ax.tick_params(labelcolor=\"w\", top=\"off\", bottom=\"off\", left=\"off\", right=\"off\", labelbottom=\"off\", labelleft=\"off\")\n",
    "bg_ax.set_ylabel(y_label, labelpad=30)\n",
    "bg_ax.patch.set_alpha(0)\n",
    "\n",
    "# initialize single plots\n",
    "axes = []\n",
    "axes.append(fig.add_subplot(gs0[0]))\n",
    "axes.append(fig.add_subplot(gs1[0, 0]))\n",
    "axes.append(fig.add_subplot(gs1[0, 1], sharey=axes[1]))\n",
    "axes.append(fig.add_subplot(gs1[1, 0]))\n",
    "axes.append(fig.add_subplot(gs1[1, 1]))\n",
    "plt.setp(axes[1].get_xticklabels(), visible=False)\n",
    "plt.setp(axes[2].get_xticklabels(), visible=False)\n",
    "plt.setp(axes[2].get_yticklabels(), visible=False)\n",
    "plt.setp(axes[4].get_yticklabels(), visible=False)\n",
    "\n",
    "# plot\n",
    "for l, label in enumerate(scores_mean[\"Labels\"].values):\n",
    "    \n",
    "    for m, method in enumerate(scores_mean[\"Methods\"].values):\n",
    "        axes[l].plot(scores_mean[\"Epochs\"].values,\n",
    "                     scores_mean.loc[method, :, label, show_metric],\n",
    "                     color=colors[m],\n",
    "                     linestyle=styles[m],\n",
    "                     linewidth=linewidth)\n",
    "    \n",
    "    axes[l].set_xlim(scores_mean[\"Epochs\"].values[0], scores_mean[\"Epochs\"].values[-1])\n",
    "    if l == 0:\n",
    "        axes[l].set_ylim(0, 1)\n",
    "    else:\n",
    "        axes[l].set_ylim(0.01, 1)\n",
    "    \n",
    "    axes[l].fill_between(scores_mean[\"Epochs\"].values,\n",
    "                         scores_mean.loc[show_std_for_method, :, label, show_metric] -\\\n",
    "                         scores_std.loc[show_std_for_method, :, label, show_metric],\n",
    "                         scores_mean.loc[show_std_for_method, :, label, show_metric] +\\\n",
    "                         scores_std.loc[show_std_for_method, :, label, show_metric],\n",
    "                         linewidth=0,\n",
    "                         alpha=std_alpha,\n",
    "                         facecolor=std_color)\n",
    "    \n",
    "    axes[l].text(text_positions[l][0], text_positions[l][1], r\"{}\".format(label),\n",
    "                 ha=text_positions[l][2], multialignment=text_positions[l][2])\n",
    "    \n",
    "axes[0].set_xlabel(\"Epoch\", labelpad=10)\n",
    "axes[0].set_ylabel(y_label)\n",
    "\n",
    "# legend in first subplot\n",
    "lines = []\n",
    "for m, method in enumerate(scores_mean[\"Methods\"].values):\n",
    "    lines.append(mpl.lines.Line2D([], [], color=colors[m], linestyle=styles[m], linewidth=linewidth))\n",
    "labels = list(scores_mean[\"Methods\"].values)\n",
    "labels = map(lambda x: x[0] + \" - \" + x[1], zip(labels, method_explanations))\n",
    "axes[0].legend(handles=lines, labels=labels, loc=4, frameon=False, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "epoch = 19\n",
    "bar_width = 0.9\n",
    "significance_threshold = 0.001\n",
    "significance_dash_length = 0.05\n",
    "significance_spacing = 0.1\n",
    "significance_text_spacing = 0.05\n",
    "with_error = True\n",
    "color_low_diff = \"k\"\n",
    "color_high_diff = \"k\"\n",
    "lower_lim = 0\n",
    "\n",
    "# initialize figure and gridspec\n",
    "fig = plt.figure(figsize=(8.27, 10))\n",
    "gs0 = mpl.gridspec.GridSpec(2, 1, hspace=0.3)\n",
    "gs1 = mpl.gridspec.GridSpecFromSubplotSpec(2, 2, subplot_spec=gs0[1], wspace=0, hspace=0)\n",
    "gs0.update(left=0.1, right=0.95, top=0.98)\n",
    "\n",
    "# invisible plot that carries labels\n",
    "bg_ax = fig.add_subplot(gs0[1])\n",
    "for s in bg_ax.spines:\n",
    "    bg_ax.spines[s].set_color(\"none\")\n",
    "bg_ax.tick_params(labelcolor=\"w\", top=\"off\", bottom=\"off\", left=\"off\", right=\"off\", labelbottom=\"off\", labelleft=\"off\")\n",
    "bg_ax.set_ylabel(y_label, labelpad=35)\n",
    "bg_ax.patch.set_alpha(0)\n",
    "\n",
    "# create empty plots\n",
    "axes = []\n",
    "axes.append(fig.add_subplot(gs0[0]))\n",
    "axes.append(fig.add_subplot(gs1[0, 0]))\n",
    "axes.append(fig.add_subplot(gs1[0, 1], sharey=axes[1]))\n",
    "axes.append(fig.add_subplot(gs1[1, 0]))\n",
    "axes.append(fig.add_subplot(gs1[1, 1]))\n",
    "\n",
    "# hide axes for subplots\n",
    "plt.setp(axes[1].get_xticklabels(), visible=False)\n",
    "plt.setp(axes[2].get_xticklabels(), visible=False)\n",
    "plt.setp(axes[2].get_yticklabels(), visible=False)\n",
    "plt.setp(axes[4].get_yticklabels(), visible=False)\n",
    "\n",
    "axes[0].set_ylabel(y_label)\n",
    "\n",
    "# plot bars\n",
    "for l, label in enumerate(scores_mean[\"Labels\"].values):\n",
    "    \n",
    "    if l == 0:\n",
    "        current_lw = linewidth\n",
    "    else:\n",
    "        current_lw = linewidth / 2.\n",
    "    \n",
    "    # get standard error of the mean\n",
    "    current_error = np.copy(scores_std.loc[:, epoch, label, show_metric].values)\n",
    "    current_error /= np.sqrt(len(scores[\"Subjects\"]))\n",
    "    \n",
    "    # plot bars\n",
    "    current_bars = axes[l].bar(range(len(scores_mean[\"Methods\"])),\n",
    "                               scores_mean.loc[:, epoch, label, show_metric],\n",
    "                               width=bar_width,\n",
    "                               yerr=current_error,\n",
    "                               error_kw=dict(lw=current_lw, ecolor=\"k\", capthick=current_lw, capsize=3*current_lw))\n",
    "    \n",
    "    # set bar colors and plot errors\n",
    "    for m in range(len(scores_mean[\"Methods\"])):\n",
    "        current_bars[m].set_color(colors[m])\n",
    "        \n",
    "    axes[l].set_xlim(axes[l].get_xlim()[0] - 0.5*bar_width, axes[l].get_xlim()[1] + 0.5*bar_width)\n",
    "    axes[l].set_xticks(np.arange(len(scores_mean[\"Methods\"])) + 0.5*bar_width)\n",
    "    axes[l].set_xticklabels(scores_mean[\"Methods\"].values, rotation=35, ha=\"right\")\n",
    "    axes[l].set_yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "# highlight significant differences\n",
    "max_height_all = 0\n",
    "for l, label in enumerate(scores_mean[\"Labels\"].values):\n",
    "    \n",
    "    if l == 0:\n",
    "        current_lw = linewidth\n",
    "    else:\n",
    "        current_lw = linewidth / 2.\n",
    "    \n",
    "    max_height = 0\n",
    "    current_height = np.max(scores_mean.loc[:, epoch, label, show_metric]) + significance_spacing    \n",
    "    indices_sorted = np.argsort(scores_mean.loc[:, epoch, label, show_metric].values)\n",
    "    if with_error:\n",
    "        current_height += scores_std.loc[:, epoch, label, show_metric].values[indices_sorted[-1]]\n",
    "    \n",
    "    # compare all pairs\n",
    "    for i1, index1 in enumerate(indices_sorted[:-1]):\n",
    "        for index2 in indices_sorted[i1+1:]:\n",
    "            \n",
    "            method1 = scores_mean[\"Methods\"].values[index1]\n",
    "            method2 = scores_mean[\"Methods\"].values[index2]\n",
    "            \n",
    "            comp1 = \"{} v {}\".format(method1, method2)\n",
    "            comp2 = \"{} v {}\".format(method2, method1)\n",
    "            \n",
    "            try:\n",
    "                current_p = test_scores.loc[comp1, epoch, label, show_metric, \"p\"].values\n",
    "                current_stat = test_scores.loc[comp1, epoch, label, show_metric, \"Statistic\"].values\n",
    "            except KeyError:\n",
    "                current_p = test_scores.loc[comp2, epoch, label, show_metric, \"p\"].values\n",
    "                current_stat = test_scores.loc[comp2, epoch, label, show_metric, \"Statistic\"].values\n",
    "                \n",
    "            if current_p < significance_threshold:\n",
    "                \n",
    "                dash_position = index1 + 0.5*bar_width\n",
    "                \n",
    "                if index1 < index2:\n",
    "                    current_color = color_low_diff\n",
    "                else:\n",
    "                    current_color = color_high_diff\n",
    "\n",
    "                axes[l].plot([index1 + 0.5*bar_width, index2 + 0.5*bar_width],\n",
    "                             [current_height, current_height],\n",
    "                             color=current_color,\n",
    "                             linewidth=current_lw,\n",
    "                             solid_capstyle=\"round\")             \n",
    "                axes[l].plot([dash_position, dash_position],\n",
    "                             [current_height, current_height - significance_dash_length],\n",
    "                             color=current_color,\n",
    "                             linewidth=current_lw,\n",
    "                             solid_capstyle=\"round\")\n",
    "                \n",
    "                current_height += significance_spacing\n",
    "                max_height = max(max_height, current_height)\n",
    "                \n",
    "    max_height_all = max(max_height_all, max_height)\n",
    "\n",
    "# adjust y limits\n",
    "for l, label in enumerate(scores_mean[\"Labels\"].values):\n",
    "    axes[l].set_ylim(lower_lim, max_height_all)\n",
    "\n",
    "# display label names\n",
    "text_positions = [\n",
    "    (5, max_height_all - 0.15, \"right\"),\n",
    "    (0, max_height_all - 0.25, \"left\"),\n",
    "    (5, max_height_all - 0.25, \"right\"),\n",
    "    (0, max_height_all - 0.25, \"left\"),\n",
    "    (0, max_height_all - 0.25, \"left\"),\n",
    "]\n",
    "for l, label in enumerate(scores_mean[\"Labels\"].values):\n",
    "    axes[l].text(text_positions[l][0], text_positions[l][1], label, ha=text_positions[l][2],\n",
    "                 multialignment=text_positions[l][2])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
